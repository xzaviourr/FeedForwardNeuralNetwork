# -*- coding: utf-8 -*-
"""10.93_maa_ki_chu.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0jBoHafgohCoq438MvdxgudpFKgr3wn
"""

# CONNECTING GOOGLE DRIVE FOR DATASET
from google.colab import drive
drive.mount("/content/drive/")
DATASET_PATH = "/content/drive/My Drive/Colab Notebooks/Datasets/Feed Forward Neural Network/"

import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

np.random.seed(42)
NUM_FEATS = 29  # Number of features

def minmaxscaler(data):
  data=(data-data.min(axis=0))/(data.max(axis=0)-data.min(axis=0))
  return pd.DataFrame(data)

def read_data(file_name):
  data = pd.read_csv(f"{DATASET_PATH}{file_name}", header=0)
  train_Y = data['1'] # First column is the labels column
  data = data.drop(['1'], axis=1)

  train_X = minmaxscaler(data)
  return train_X, pd.DataFrame(train_Y)

def apply_pca(train_X):
  current_features = train_X.T
  cov_matrix = np.cov(current_features)
  values, vectors = np.linalg.eig(cov_matrix)
  explained_variances = []
  for i in range(len(values)):
      explained_variances.append(values[i] / np.sum(values))
  
  current_features = train_X.T
  cov_matrix = np.cov(current_features)
  values, vectors = np.linalg.eig(cov_matrix)
  explained_variances = []
  for i in range(len(values)):
      explained_variances.append(values[i] / np.sum(values))

  baseline_coverage = 0.92
  current_coverage = 0
  reduced_feature_count = 0
  for ev in range(len(explained_variances)):
    current_coverage += explained_variances[ev]
    if current_coverage > baseline_coverage:
      reduced_feature_count = ev
      break
  
  print(reduced_feature_count)
  NUM_FEATS = reduced_feature_count
  
  return vectors

class Net(object):
  def __init__(self, num_layers, num_units):
    self.num_layers = num_layers
    self.num_units = num_units

    self.biases = []
    self.weights = []

    for i in range(num_layers):
      if i == 0:  # Input Layer
        self.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, num_units)))
      else: # Hidden Layer
        self.weights.append(np.random.uniform(-1, 1, size=(num_units, num_units)))
      self.biases.append(np.random.uniform(-1, 1, size=(num_units, 1)))

    # Output Layer
    self.weights.append(np.random.uniform(-1, 1, size=(num_units, 1)))
    self.biases.append(np.random.uniform(-1, 1, size=(1, 1)))

  def __call__(self, train_X):
    self.aggregates = list()
    self.activations = list()
    
    layer_input = train_X
    for layer in range(self.num_layers):
      aggregate = np.dot(layer_input, self.weights[layer]) + self.biases[layer].T
      activation = self.relu(aggregate)
      self.aggregates.append(aggregate)
      self.activations.append(activation)
      layer_input = activation
    # Output layer
    aggregate = np.dot(layer_input, self.weights[layer+1]) + self.biases[layer+1].T
    self.aggregates.append(aggregate)
    self.activations.append(aggregate)
    
    return aggregate

  def relu(self, input_matrix):
    return np.maximum(input_matrix, 0)

  def relu_grad(self, input_matrix):
    return input_matrix > 0

  def backward(self, x, y, y_hat, lamda):
    weight_gradients = [None]*(self.num_layers + 1)
    node_gradients = [None]*(self.num_layers + 1)

    regularized_component = 0
    weight_sum = 0
    for w in self.weights:
      regularized_component += np.sqrt(np.sum(np.square(w)))
      weight_sum += np.sum(np.abs(w))

    # Calculating node gradients
    for layer in range(self.num_layers, -1, -1):
      if layer == self.num_layers:
        node_gradients[layer] = 2*np.subtract(y_hat, y) 
      else:
        temp = np.dot(node_gradients[layer+1], self.weights[layer+1].T)
        node_gradients[layer] = np.multiply(temp, self.relu_grad(self.aggregates[layer])) 
      
    for layer in range(self.num_layers, 0, -1):
      weight_gradients[layer] = np.einsum('ij,ik->ijk', self.activations[layer-1], node_gradients[layer]) + 2 * lamda * self.weights[layer]

    layer = layer - 1
    weight_gradients[layer] = np.einsum('ij,ik->ijk', x, node_gradients[layer]) + 2 * lamda * self.weights[layer]
    
    for layer in range(self.num_layers, -1, -1):
      weight_gradients[layer] = np.mean(weight_gradients[layer], axis = 0)

    for layer in range(self.num_layers, -1, -1):
      node_gradients[layer] = np.sum(node_gradients[layer], axis = 0)
      if type(node_gradients[layer]) != np.ndarray:
        node_gradients[layer] = node_gradients[layer].to_numpy()

    for l in range(len(node_gradients)):
      node_gradients[l] = node_gradients[l].reshape(-1,1)
    return weight_gradients, node_gradients

class Optimizer(object):
  def __init__(self, learning_rate):
    self.learning_rate = learning_rate

  def step(self, weights, delta_weights, biases, delta_biases):
    for layer in range(len(weights)):
      delta_wt = self.learning_rate*delta_weights[layer]
      weights[layer] = np.subtract(weights[layer], delta_wt)

    for layer in range(len(biases)):
      delta_bias = self.learning_rate*delta_biases[layer]
      biases[layer] = np.subtract(biases[layer], delta_bias)
    return weights, biases

class AdamOptimizer(object):

    def __init__(self, learning_rate):
        self.m = []
        self.v = []
        self.t = 1
        self.learning_rate = learning_rate
        
    def step(self, weights, delta_weights , biases , delta_biases, beta1 = 0.9, beta2 = 0.999):
        """ Adam optimizer, bias correction is implemented. """
        if(self.t==1):
          for layer in range(len(weights)):
            self.m.append(np.random.uniform(0.0, 0.0, size=delta_weights[layer].shape))
            self.v.append(np.random.uniform(0.0, 0.0, size=delta_weights[layer].shape))
        # updated_params = []
        
        for  layer in range(len(weights)):
          
          self.m[layer] = beta1 * self.m[layer] + (1-beta1) * delta_weights[layer]       
          self.v[layer] = beta2 * self.v[layer] + (1-beta2) * delta_weights[layer] **2
          m_corrected = self.m[layer] / (1-beta1**self.t)
          v_corrected = self.v[layer] / (1-beta2**self.t)
          weights[layer] += -self.learning_rate * m_corrected / (np.sqrt(v_corrected) + 1e-8)
        
        self.t +=1
        
        for layer in range(len(weights)):
          delta_bias = self.learning_rate*delta_biases[layer]
     
          biases[layer] = np.subtract(biases[layer], delta_bias)
        return weights,biases

def loss_mse(y, y_hat):
  return (((np.subtract(y, y_hat)**2).sum())/y.shape[0])**(0.5)

raw_test_X = np.array(pd.read_csv(DATASET_PATH + "test.csv"))
raw_test_X = minmaxscaler(raw_test_X)

raw_train_X, train_Y = read_data('train.csv')
raw_dev_X, dev_Y = read_data("dev.csv")

columns = [x for x in range(1, 91)]
columns = columns
raw_test_X.columns.values[:] = columns
raw_train_X.columns.values[:] = columns
raw_dev_X.columns.values[:] = columns

db = pd.concat([raw_train_X, raw_dev_X, raw_test_X], axis=0)

db.head()

vectors = apply_pca(db)

train_X = pd.DataFrame()
for i in range(NUM_FEATS):
    train_X[f'Feature{i}'] = raw_train_X.dot(vectors.T[i])

dev_X = pd.DataFrame()
for i in range(NUM_FEATS):
    dev_X[f'Feature{i}'] = raw_dev_X.dot(vectors.T[i])

test_X = pd.DataFrame()
for i in range(NUM_FEATS):
    test_X[f'Feature{i}'] = raw_test_X.dot(vectors.T[i])

FFNN = Net(1, 256) # 2 hidden layers, 64 nodes each
learning_rate = 10**-3
lamda = 0.0005

optimizer = AdamOptimizer(learning_rate)
no_of_epochs = 500

best_weights = []
best_biases = []
best_error = 10**9
batch_size = 16
flag = 0

no_of_samples = train_X.shape[0]

dev_loss = []
train_loss = []

for i in range(no_of_epochs):
  no_of_batches  = no_of_samples // batch_size
  batch_weight_gradients = []
  batch_bias_gradients = []
  for bch in range(no_of_batches):
    batch_X = train_X.iloc[bch*batch_size:(bch+1)*batch_size].copy()
    batch_Y = train_Y.iloc[bch*batch_size:(bch+1)*batch_size].copy()

    pred_Y = pd.DataFrame(FFNN(batch_X))
    del_w, del_b = FFNN.backward(batch_X, batch_Y, pred_Y, lamda)
    if batch_weight_gradients == []:
      batch_weight_gradients = del_w
      batch_bias_gradients = del_b
    else:
      for layer in range(len(batch_weight_gradients)):
        batch_weight_gradients[layer] = np.add(batch_weight_gradients[layer], del_w[layer])
      for layer in range(len(batch_bias_gradients)):
        batch_bias_gradients[layer] = np.add(batch_bias_gradients[layer], del_b[layer])

  for layer in range(len(batch_weight_gradients)):
    batch_weight_gradients[layer] /= no_of_batches
  for layer in range(len(batch_bias_gradients)):
    batch_bias_gradients[layer] /= no_of_batches

  pred_Y = pd.DataFrame(FFNN(train_X))
  mse_error = loss_mse(train_Y, pred_Y).iloc[0]
  train_loss.append(mse_error)

  predictions = FFNN(dev_X)
  dev_mse = loss_mse(dev_Y,predictions).iloc[0]
  dev_loss.append(dev_mse)

  new_w, new_b = optimizer.step(FFNN.weights, batch_weight_gradients, FFNN.biases, batch_bias_gradients)
  print(f"EPOCH {i} : RMSE ERROR : {mse_error}")

predictions = FFNN(dev_X)
dev_mse = loss_mse(dev_Y,predictions).iloc[0]
print(f"TRAIN SET : {mse_error} | DEV SET : {dev_mse}")
print(f"TRAIN SET * DEV SET : {mse_error*dev_mse}")

tst = FFNN(test_X)
df = pd.DataFrame(tst)
df.index = df.index+1
df.to_csv('pred.csv', header=['Predictions'], index=True, index_label='Id')
print(tst)

tst.min()

tst.max()

