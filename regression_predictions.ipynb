{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkQBffau_86M",
        "outputId": "a4f81ffb-dc37-40e8-a697-fef1e7271b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# CONNECTING GOOGLE DRIVE FOR DATASET\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "DATASET_PATH = \"/content/drive/My Drive/Colab Notebooks/Datasets/Feed Forward Neural Network/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "NUM_FEATS = 29  # Number of features"
      ],
      "metadata": {
        "id": "xv-NPydmACsk"
      },
      "execution_count": 777,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def minmaxscaler(data):\n",
        "  data=(data-data.min(axis=0))/(data.max(axis=0)-data.min(axis=0))\n",
        "  return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "4qA9D7QkeXAv"
      },
      "execution_count": 778,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(file_name):\n",
        "  data = pd.read_csv(f\"{DATASET_PATH}{file_name}\", header=0)\n",
        "  train_Y = data['1'] # First column is the labels column\n",
        "  data = data.drop(['1'], axis=1)\n",
        "\n",
        "  train_X = minmaxscaler(data)\n",
        "  return train_X, pd.DataFrame(train_Y)"
      ],
      "metadata": {
        "id": "EPyRuwGhAEOv"
      },
      "execution_count": 779,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(train_X):\n",
        "  current_features = train_X.T\n",
        "  cov_matrix = np.cov(current_features)\n",
        "  values, vectors = np.linalg.eig(cov_matrix)\n",
        "  explained_variances = []\n",
        "  for i in range(len(values)):\n",
        "      explained_variances.append(values[i] / np.sum(values))\n",
        "  \n",
        "  current_features = train_X.T\n",
        "  cov_matrix = np.cov(current_features)\n",
        "  values, vectors = np.linalg.eig(cov_matrix)\n",
        "  explained_variances = []\n",
        "  for i in range(len(values)):\n",
        "      explained_variances.append(values[i] / np.sum(values))\n",
        "\n",
        "  baseline_coverage = 0.92\n",
        "  current_coverage = 0\n",
        "  reduced_feature_count = 0\n",
        "  for ev in range(len(explained_variances)):\n",
        "    current_coverage += explained_variances[ev]\n",
        "    if current_coverage > baseline_coverage:\n",
        "      reduced_feature_count = ev\n",
        "      break\n",
        "  \n",
        "  print(reduced_feature_count)\n",
        "  NUM_FEATS = reduced_feature_count\n",
        "  \n",
        "  return vectors"
      ],
      "metadata": {
        "id": "OYLbDKrlod2g"
      },
      "execution_count": 780,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(object):\n",
        "  def __init__(self, num_layers, num_units):\n",
        "    self.num_layers = num_layers\n",
        "    self.num_units = num_units\n",
        "\n",
        "    self.biases = []\n",
        "    self.weights = []\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      if i == 0:  # Input Layer\n",
        "        self.weights.append(np.random.uniform(-1, 1, size=(NUM_FEATS, num_units)))\n",
        "      else: # Hidden Layer\n",
        "        self.weights.append(np.random.uniform(-1, 1, size=(num_units, num_units)))\n",
        "      self.biases.append(np.random.uniform(-1, 1, size=(num_units, 1)))\n",
        "\n",
        "    # Output Layer\n",
        "    self.weights.append(np.random.uniform(-1, 1, size=(num_units, 1)))\n",
        "    self.biases.append(np.random.uniform(-1, 1, size=(1, 1)))\n",
        "\n",
        "  def __call__(self, train_X):\n",
        "    self.aggregates = list()\n",
        "    self.activations = list()\n",
        "    \n",
        "    layer_input = train_X\n",
        "    for layer in range(self.num_layers):\n",
        "      aggregate = np.dot(layer_input, self.weights[layer]) + self.biases[layer].T\n",
        "      activation = self.relu(aggregate)\n",
        "      self.aggregates.append(aggregate)\n",
        "      self.activations.append(activation)\n",
        "      layer_input = activation\n",
        "    # Output layer\n",
        "    aggregate = np.dot(layer_input, self.weights[layer+1]) + self.biases[layer+1].T\n",
        "    self.aggregates.append(aggregate)\n",
        "    self.activations.append(aggregate)\n",
        "    \n",
        "    return aggregate\n",
        "\n",
        "  def relu(self, input_matrix):\n",
        "    return np.maximum(input_matrix, 0)\n",
        "\n",
        "  def relu_grad(self, input_matrix):\n",
        "    return input_matrix > 0\n",
        "\n",
        "  def backward(self, x, y, y_hat, lamda):\n",
        "    weight_gradients = [None]*(self.num_layers + 1)\n",
        "    node_gradients = [None]*(self.num_layers + 1)\n",
        "\n",
        "    regularized_component = 0\n",
        "    weight_sum = 0\n",
        "    for w in self.weights:\n",
        "      regularized_component += np.sqrt(np.sum(np.square(w)))\n",
        "      weight_sum += np.sum(np.abs(w))\n",
        "\n",
        "    # Calculating node gradients\n",
        "    for layer in range(self.num_layers, -1, -1):\n",
        "      if layer == self.num_layers:\n",
        "        node_gradients[layer] = 2*np.subtract(y_hat, y) \n",
        "      else:\n",
        "        temp = np.dot(node_gradients[layer+1], self.weights[layer+1].T)\n",
        "        node_gradients[layer] = np.multiply(temp, self.relu_grad(self.aggregates[layer])) \n",
        "      \n",
        "    for layer in range(self.num_layers, 0, -1):\n",
        "      weight_gradients[layer] = np.einsum('ij,ik->ijk', self.activations[layer-1], node_gradients[layer]) + 2 * lamda * self.weights[layer]\n",
        "\n",
        "    layer = layer - 1\n",
        "    weight_gradients[layer] = np.einsum('ij,ik->ijk', x, node_gradients[layer]) + 2 * lamda * self.weights[layer]\n",
        "    \n",
        "    for layer in range(self.num_layers, -1, -1):\n",
        "      weight_gradients[layer] = np.mean(weight_gradients[layer], axis = 0)\n",
        "\n",
        "    for layer in range(self.num_layers, -1, -1):\n",
        "      node_gradients[layer] = np.sum(node_gradients[layer], axis = 0)\n",
        "      if type(node_gradients[layer]) != np.ndarray:\n",
        "        node_gradients[layer] = node_gradients[layer].to_numpy()\n",
        "\n",
        "    for l in range(len(node_gradients)):\n",
        "      node_gradients[l] = node_gradients[l].reshape(-1,1)\n",
        "    return weight_gradients, node_gradients"
      ],
      "metadata": {
        "id": "hRL-xk-1AF6A"
      },
      "execution_count": 828,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer(object):\n",
        "  def __init__(self, learning_rate):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def step(self, weights, delta_weights, biases, delta_biases):\n",
        "    for layer in range(len(weights)):\n",
        "      delta_wt = self.learning_rate*delta_weights[layer]\n",
        "      weights[layer] = np.subtract(weights[layer], delta_wt)\n",
        "\n",
        "    for layer in range(len(biases)):\n",
        "      delta_bias = self.learning_rate*delta_biases[layer]\n",
        "      biases[layer] = np.subtract(biases[layer], delta_bias)\n",
        "    return weights, biases"
      ],
      "metadata": {
        "id": "FPpIvPfGMrrj"
      },
      "execution_count": 829,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamOptimizer(object):\n",
        "\n",
        "    def __init__(self, learning_rate):\n",
        "        self.m = []\n",
        "        self.v = []\n",
        "        self.t = 1\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "    def step(self, weights, delta_weights , biases , delta_biases, beta1 = 0.9, beta2 = 0.999):\n",
        "        \"\"\" Adam optimizer, bias correction is implemented. \"\"\"\n",
        "        if(self.t==1):\n",
        "          for layer in range(len(weights)):\n",
        "            self.m.append(np.random.uniform(0.0, 0.0, size=delta_weights[layer].shape))\n",
        "            self.v.append(np.random.uniform(0.0, 0.0, size=delta_weights[layer].shape))\n",
        "        # updated_params = []\n",
        "        \n",
        "        for  layer in range(len(weights)):\n",
        "          \n",
        "          self.m[layer] = beta1 * self.m[layer] + (1-beta1) * delta_weights[layer]       \n",
        "          self.v[layer] = beta2 * self.v[layer] + (1-beta2) * delta_weights[layer] **2\n",
        "          m_corrected = self.m[layer] / (1-beta1**self.t)\n",
        "          v_corrected = self.v[layer] / (1-beta2**self.t)\n",
        "          weights[layer] += -self.learning_rate * m_corrected / (np.sqrt(v_corrected) + 1e-8)\n",
        "        \n",
        "        self.t +=1\n",
        "        \n",
        "        for layer in range(len(weights)):\n",
        "          delta_bias = self.learning_rate*delta_biases[layer]\n",
        "     \n",
        "          biases[layer] = np.subtract(biases[layer], delta_bias)\n",
        "        return weights,biases"
      ],
      "metadata": {
        "id": "AKFukBQhNYas"
      },
      "execution_count": 830,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_mse(y, y_hat):\n",
        "  return (((np.subtract(y, y_hat)**2).sum())/y.shape[0])**(0.5)"
      ],
      "metadata": {
        "id": "Vtqm8opTMtP5"
      },
      "execution_count": 831,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_test_X = np.array(pd.read_csv(DATASET_PATH + \"test.csv\"))\n",
        "raw_test_X = minmaxscaler(raw_test_X)\n",
        "\n",
        "raw_train_X, train_Y = read_data('train.csv')\n",
        "raw_dev_X, dev_Y = read_data(\"dev.csv\")\n",
        "\n",
        "columns = [x for x in range(1, 91)]\n",
        "columns = columns\n",
        "raw_test_X.columns.values[:] = columns\n",
        "raw_train_X.columns.values[:] = columns\n",
        "raw_dev_X.columns.values[:] = columns\n",
        "\n",
        "db = pd.concat([raw_train_X, raw_dev_X, raw_test_X], axis=0)"
      ],
      "metadata": {
        "id": "NgiFUqzgLxMF"
      },
      "execution_count": 832,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "60P3pbJqgJXj",
        "outputId": "cd72dc94-4da2-4e1b-b835-79c2dcbb6547"
      },
      "execution_count": 833,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         1         2         3         4         5         6         7   \\\n",
              "0  0.625066  0.518548  0.591790  0.512467  0.315919  0.606895  0.524523   \n",
              "1  0.391954  0.377018  0.418961  0.345425  0.429244  0.359135  0.533448   \n",
              "2  0.552253  0.633584  0.437503  0.525843  0.452884  0.443956  0.594289   \n",
              "3  0.761643  0.565835  0.554013  0.451897  0.417237  0.301431  0.595046   \n",
              "4  0.606655  0.425655  0.583708  0.442717  0.478696  0.407195  0.529120   \n",
              "\n",
              "         8         9         10  ...        81        82        83        84  \\\n",
              "0  0.414843  0.528907  0.544377  ...  0.262914  0.350085  0.334135  0.431003   \n",
              "1  0.450449  0.351191  0.505538  ...  0.355628  0.370124  0.304988  0.489212   \n",
              "2  0.525178  0.388741  0.630555  ...  0.246077  0.463615  0.325095  0.503980   \n",
              "3  0.436198  0.395769  0.500972  ...  0.243847  0.406358  0.290330  0.483724   \n",
              "4  0.434133  0.358751  0.417719  ...  0.302174  0.368794  0.299064  0.498369   \n",
              "\n",
              "         85        86        87        88        89        90  \n",
              "0  0.387217  0.669793  0.507545  0.499780  0.765121  0.284739  \n",
              "1  0.427288  0.707801  0.559043  0.473614  0.754003  0.284978  \n",
              "2  0.552231  0.672552  0.582417  0.521172  0.724217  0.279617  \n",
              "3  0.464408  0.698157  0.585269  0.487985  0.740129  0.267115  \n",
              "4  0.493770  0.687865  0.580101  0.458438  0.734952  0.262689  \n",
              "\n",
              "[5 rows x 90 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-005bc4a0-0ea0-47b4-8a5c-6877f0757063\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>...</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.625066</td>\n",
              "      <td>0.518548</td>\n",
              "      <td>0.591790</td>\n",
              "      <td>0.512467</td>\n",
              "      <td>0.315919</td>\n",
              "      <td>0.606895</td>\n",
              "      <td>0.524523</td>\n",
              "      <td>0.414843</td>\n",
              "      <td>0.528907</td>\n",
              "      <td>0.544377</td>\n",
              "      <td>...</td>\n",
              "      <td>0.262914</td>\n",
              "      <td>0.350085</td>\n",
              "      <td>0.334135</td>\n",
              "      <td>0.431003</td>\n",
              "      <td>0.387217</td>\n",
              "      <td>0.669793</td>\n",
              "      <td>0.507545</td>\n",
              "      <td>0.499780</td>\n",
              "      <td>0.765121</td>\n",
              "      <td>0.284739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.391954</td>\n",
              "      <td>0.377018</td>\n",
              "      <td>0.418961</td>\n",
              "      <td>0.345425</td>\n",
              "      <td>0.429244</td>\n",
              "      <td>0.359135</td>\n",
              "      <td>0.533448</td>\n",
              "      <td>0.450449</td>\n",
              "      <td>0.351191</td>\n",
              "      <td>0.505538</td>\n",
              "      <td>...</td>\n",
              "      <td>0.355628</td>\n",
              "      <td>0.370124</td>\n",
              "      <td>0.304988</td>\n",
              "      <td>0.489212</td>\n",
              "      <td>0.427288</td>\n",
              "      <td>0.707801</td>\n",
              "      <td>0.559043</td>\n",
              "      <td>0.473614</td>\n",
              "      <td>0.754003</td>\n",
              "      <td>0.284978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.552253</td>\n",
              "      <td>0.633584</td>\n",
              "      <td>0.437503</td>\n",
              "      <td>0.525843</td>\n",
              "      <td>0.452884</td>\n",
              "      <td>0.443956</td>\n",
              "      <td>0.594289</td>\n",
              "      <td>0.525178</td>\n",
              "      <td>0.388741</td>\n",
              "      <td>0.630555</td>\n",
              "      <td>...</td>\n",
              "      <td>0.246077</td>\n",
              "      <td>0.463615</td>\n",
              "      <td>0.325095</td>\n",
              "      <td>0.503980</td>\n",
              "      <td>0.552231</td>\n",
              "      <td>0.672552</td>\n",
              "      <td>0.582417</td>\n",
              "      <td>0.521172</td>\n",
              "      <td>0.724217</td>\n",
              "      <td>0.279617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.761643</td>\n",
              "      <td>0.565835</td>\n",
              "      <td>0.554013</td>\n",
              "      <td>0.451897</td>\n",
              "      <td>0.417237</td>\n",
              "      <td>0.301431</td>\n",
              "      <td>0.595046</td>\n",
              "      <td>0.436198</td>\n",
              "      <td>0.395769</td>\n",
              "      <td>0.500972</td>\n",
              "      <td>...</td>\n",
              "      <td>0.243847</td>\n",
              "      <td>0.406358</td>\n",
              "      <td>0.290330</td>\n",
              "      <td>0.483724</td>\n",
              "      <td>0.464408</td>\n",
              "      <td>0.698157</td>\n",
              "      <td>0.585269</td>\n",
              "      <td>0.487985</td>\n",
              "      <td>0.740129</td>\n",
              "      <td>0.267115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.606655</td>\n",
              "      <td>0.425655</td>\n",
              "      <td>0.583708</td>\n",
              "      <td>0.442717</td>\n",
              "      <td>0.478696</td>\n",
              "      <td>0.407195</td>\n",
              "      <td>0.529120</td>\n",
              "      <td>0.434133</td>\n",
              "      <td>0.358751</td>\n",
              "      <td>0.417719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.302174</td>\n",
              "      <td>0.368794</td>\n",
              "      <td>0.299064</td>\n",
              "      <td>0.498369</td>\n",
              "      <td>0.493770</td>\n",
              "      <td>0.687865</td>\n",
              "      <td>0.580101</td>\n",
              "      <td>0.458438</td>\n",
              "      <td>0.734952</td>\n",
              "      <td>0.262689</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 90 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-005bc4a0-0ea0-47b4-8a5c-6877f0757063')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-005bc4a0-0ea0-47b4-8a5c-6877f0757063 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-005bc4a0-0ea0-47b4-8a5c-6877f0757063');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 833
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = apply_pca(db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN2pStmeMcld",
        "outputId": "26b2a39b-7e4f-4cb1-aa49-2de3f5a400c8"
      },
      "execution_count": 834,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = pd.DataFrame()\n",
        "for i in range(NUM_FEATS):\n",
        "    train_X[f'Feature{i}'] = raw_train_X.dot(vectors.T[i])\n",
        "\n",
        "dev_X = pd.DataFrame()\n",
        "for i in range(NUM_FEATS):\n",
        "    dev_X[f'Feature{i}'] = raw_dev_X.dot(vectors.T[i])\n",
        "\n",
        "test_X = pd.DataFrame()\n",
        "for i in range(NUM_FEATS):\n",
        "    test_X[f'Feature{i}'] = raw_test_X.dot(vectors.T[i])"
      ],
      "metadata": {
        "id": "60Zm-JOTMrHz"
      },
      "execution_count": 835,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FFNN = Net(1, 256) # 2 hidden layers, 64 nodes each\n",
        "learning_rate = 10**-3\n",
        "lamda = 0.0005\n",
        "\n",
        "optimizer = AdamOptimizer(learning_rate)\n",
        "no_of_epochs = 500\n",
        "\n",
        "best_weights = []\n",
        "best_biases = []\n",
        "best_error = 10**9\n",
        "batch_size = 16\n",
        "flag = 0\n",
        "\n",
        "no_of_samples = train_X.shape[0]\n",
        "\n",
        "dev_loss = []\n",
        "train_loss = []\n",
        "\n",
        "for i in range(no_of_epochs):\n",
        "  no_of_batches  = no_of_samples // batch_size\n",
        "  batch_weight_gradients = []\n",
        "  batch_bias_gradients = []\n",
        "  for bch in range(no_of_batches):\n",
        "    batch_X = train_X.iloc[bch*batch_size:(bch+1)*batch_size].copy()\n",
        "    batch_Y = train_Y.iloc[bch*batch_size:(bch+1)*batch_size].copy()\n",
        "\n",
        "    pred_Y = pd.DataFrame(FFNN(batch_X))\n",
        "    del_w, del_b = FFNN.backward(batch_X, batch_Y, pred_Y, lamda)\n",
        "    if batch_weight_gradients == []:\n",
        "      batch_weight_gradients = del_w\n",
        "      batch_bias_gradients = del_b\n",
        "    else:\n",
        "      for layer in range(len(batch_weight_gradients)):\n",
        "        batch_weight_gradients[layer] = np.add(batch_weight_gradients[layer], del_w[layer])\n",
        "      for layer in range(len(batch_bias_gradients)):\n",
        "        batch_bias_gradients[layer] = np.add(batch_bias_gradients[layer], del_b[layer])\n",
        "\n",
        "  for layer in range(len(batch_weight_gradients)):\n",
        "    batch_weight_gradients[layer] /= no_of_batches\n",
        "  for layer in range(len(batch_bias_gradients)):\n",
        "    batch_bias_gradients[layer] /= no_of_batches\n",
        "\n",
        "  pred_Y = pd.DataFrame(FFNN(train_X))\n",
        "  mse_error = loss_mse(train_Y, pred_Y).iloc[0]\n",
        "  train_loss.append(mse_error)\n",
        "\n",
        "  predictions = FFNN(dev_X)\n",
        "  dev_mse = loss_mse(dev_Y,predictions).iloc[0]\n",
        "  dev_loss.append(dev_mse)\n",
        "\n",
        "  new_w, new_b = optimizer.step(FFNN.weights, batch_weight_gradients, FFNN.biases, batch_bias_gradients)\n",
        "  print(f\"EPOCH {i} : RMSE ERROR : {mse_error}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE2GmyXZDaV_",
        "outputId": "99f1fa18-8c3f-4f54-d439-251bb8e0ffee"
      },
      "execution_count": 848,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
            "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: Calling a ufunc on non-aligned DataFrames (or DataFrame/Series combination). Currently, the indices are ignored and the result takes the index/columns of the first DataFrame. In the future , the DataFrames/Series will be aligned before applying the ufunc.\n",
            "Convert one of the arguments to a NumPy array (eg 'ufunc(df1, np.asarray(df2)') to keep the current behaviour, or align manually (eg 'df1, df2 = df1.align(df2)') before passing to the ufunc to obtain the future behaviour and silence this warning.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 0 : RMSE ERROR : 2000.6039029367316\n",
            "EPOCH 1 : RMSE ERROR : 631.9638242030176\n",
            "EPOCH 2 : RMSE ERROR : 86.88972658803443\n",
            "EPOCH 3 : RMSE ERROR : 13.382441525843968\n",
            "EPOCH 4 : RMSE ERROR : 11.373063954304289\n",
            "EPOCH 5 : RMSE ERROR : 11.43941427942765\n",
            "EPOCH 6 : RMSE ERROR : 11.40340313414922\n",
            "EPOCH 7 : RMSE ERROR : 11.371704288274579\n",
            "EPOCH 8 : RMSE ERROR : 11.34887887381432\n",
            "EPOCH 9 : RMSE ERROR : 11.332268753754628\n",
            "EPOCH 10 : RMSE ERROR : 11.319887107381408\n",
            "EPOCH 11 : RMSE ERROR : 11.310466195488555\n",
            "EPOCH 12 : RMSE ERROR : 11.303176760469741\n",
            "EPOCH 13 : RMSE ERROR : 11.297426873148536\n",
            "EPOCH 14 : RMSE ERROR : 11.292805893276082\n",
            "EPOCH 15 : RMSE ERROR : 11.289015028786787\n",
            "EPOCH 16 : RMSE ERROR : 11.28583801358962\n",
            "EPOCH 17 : RMSE ERROR : 11.28309767078712\n",
            "EPOCH 18 : RMSE ERROR : 11.280660945843414\n",
            "EPOCH 19 : RMSE ERROR : 11.27840858120855\n",
            "EPOCH 20 : RMSE ERROR : 11.276246745617398\n",
            "EPOCH 21 : RMSE ERROR : 11.27412294926763\n",
            "EPOCH 22 : RMSE ERROR : 11.272022885944862\n",
            "EPOCH 23 : RMSE ERROR : 11.269982761145215\n",
            "EPOCH 24 : RMSE ERROR : 11.26806972717384\n",
            "EPOCH 25 : RMSE ERROR : 11.266319650806338\n",
            "EPOCH 26 : RMSE ERROR : 11.264763241631204\n",
            "EPOCH 27 : RMSE ERROR : 11.263397938471135\n",
            "EPOCH 28 : RMSE ERROR : 11.262210776417527\n",
            "EPOCH 29 : RMSE ERROR : 11.261185785941494\n",
            "EPOCH 30 : RMSE ERROR : 11.260314190582966\n",
            "EPOCH 31 : RMSE ERROR : 11.259584808672114\n",
            "EPOCH 32 : RMSE ERROR : 11.25895518939062\n",
            "EPOCH 33 : RMSE ERROR : 11.258385848608965\n",
            "EPOCH 34 : RMSE ERROR : 11.257866795737586\n",
            "EPOCH 35 : RMSE ERROR : 11.25739242072539\n",
            "EPOCH 36 : RMSE ERROR : 11.256947867157427\n",
            "EPOCH 37 : RMSE ERROR : 11.25652931626474\n",
            "EPOCH 38 : RMSE ERROR : 11.256138946045288\n",
            "EPOCH 39 : RMSE ERROR : 11.255775344878266\n",
            "EPOCH 40 : RMSE ERROR : 11.255430569600547\n",
            "EPOCH 41 : RMSE ERROR : 11.25509990515776\n",
            "EPOCH 42 : RMSE ERROR : 11.254777785427532\n",
            "EPOCH 43 : RMSE ERROR : 11.254463700397954\n",
            "EPOCH 44 : RMSE ERROR : 11.254154282777904\n",
            "EPOCH 45 : RMSE ERROR : 11.253848430451802\n",
            "EPOCH 46 : RMSE ERROR : 11.253546819450209\n",
            "EPOCH 47 : RMSE ERROR : 11.253248371390061\n",
            "EPOCH 48 : RMSE ERROR : 11.25295065213437\n",
            "EPOCH 49 : RMSE ERROR : 11.252653398454784\n",
            "EPOCH 50 : RMSE ERROR : 11.252357051809085\n",
            "EPOCH 51 : RMSE ERROR : 11.252060884158459\n",
            "EPOCH 52 : RMSE ERROR : 11.251764349012623\n",
            "EPOCH 53 : RMSE ERROR : 11.251467223731797\n",
            "EPOCH 54 : RMSE ERROR : 11.25116897186971\n",
            "EPOCH 55 : RMSE ERROR : 11.250870357200668\n",
            "EPOCH 56 : RMSE ERROR : 11.25057048369454\n",
            "EPOCH 57 : RMSE ERROR : 11.250267629497577\n",
            "EPOCH 58 : RMSE ERROR : 11.249962983953687\n",
            "EPOCH 59 : RMSE ERROR : 11.249657437655188\n",
            "EPOCH 60 : RMSE ERROR : 11.249349636108077\n",
            "EPOCH 61 : RMSE ERROR : 11.249039934732833\n",
            "EPOCH 62 : RMSE ERROR : 11.248730831198971\n",
            "EPOCH 63 : RMSE ERROR : 11.248420208793709\n",
            "EPOCH 64 : RMSE ERROR : 11.248109263862037\n",
            "EPOCH 65 : RMSE ERROR : 11.24779692568712\n",
            "EPOCH 66 : RMSE ERROR : 11.247484246100147\n",
            "EPOCH 67 : RMSE ERROR : 11.247171781529481\n",
            "EPOCH 68 : RMSE ERROR : 11.24685647494298\n",
            "EPOCH 69 : RMSE ERROR : 11.24653797180451\n",
            "EPOCH 70 : RMSE ERROR : 11.246216669785396\n",
            "EPOCH 71 : RMSE ERROR : 11.24589316812984\n",
            "EPOCH 72 : RMSE ERROR : 11.245567970191575\n",
            "EPOCH 73 : RMSE ERROR : 11.245240427209271\n",
            "EPOCH 74 : RMSE ERROR : 11.244909970436526\n",
            "EPOCH 75 : RMSE ERROR : 11.244575417498934\n",
            "EPOCH 76 : RMSE ERROR : 11.244239018761567\n",
            "EPOCH 77 : RMSE ERROR : 11.243901005320739\n",
            "EPOCH 78 : RMSE ERROR : 11.243562856617775\n",
            "EPOCH 79 : RMSE ERROR : 11.243222238885082\n",
            "EPOCH 80 : RMSE ERROR : 11.242876808166498\n",
            "EPOCH 81 : RMSE ERROR : 11.242528438659683\n",
            "EPOCH 82 : RMSE ERROR : 11.242177514567755\n",
            "EPOCH 83 : RMSE ERROR : 11.241827355950498\n",
            "EPOCH 84 : RMSE ERROR : 11.241478087031282\n",
            "EPOCH 85 : RMSE ERROR : 11.241124431144188\n",
            "EPOCH 86 : RMSE ERROR : 11.240768619612547\n",
            "EPOCH 87 : RMSE ERROR : 11.240410140569535\n",
            "EPOCH 88 : RMSE ERROR : 11.240049566673303\n",
            "EPOCH 89 : RMSE ERROR : 11.239686199626457\n",
            "EPOCH 90 : RMSE ERROR : 11.239319621843533\n",
            "EPOCH 91 : RMSE ERROR : 11.23895328161962\n",
            "EPOCH 92 : RMSE ERROR : 11.238590078349942\n",
            "EPOCH 93 : RMSE ERROR : 11.238230129747409\n",
            "EPOCH 94 : RMSE ERROR : 11.237871062914609\n",
            "EPOCH 95 : RMSE ERROR : 11.2375160481477\n",
            "EPOCH 96 : RMSE ERROR : 11.237164134901368\n",
            "EPOCH 97 : RMSE ERROR : 11.236812652551947\n",
            "EPOCH 98 : RMSE ERROR : 11.236462344655527\n",
            "EPOCH 99 : RMSE ERROR : 11.236112635564792\n",
            "EPOCH 100 : RMSE ERROR : 11.235761981883961\n",
            "EPOCH 101 : RMSE ERROR : 11.235411861998926\n",
            "EPOCH 102 : RMSE ERROR : 11.235061132094385\n",
            "EPOCH 103 : RMSE ERROR : 11.234708818038788\n",
            "EPOCH 104 : RMSE ERROR : 11.234356308990849\n",
            "EPOCH 105 : RMSE ERROR : 11.234005664271997\n",
            "EPOCH 106 : RMSE ERROR : 11.233655514897567\n",
            "EPOCH 107 : RMSE ERROR : 11.233306349479196\n",
            "EPOCH 108 : RMSE ERROR : 11.23295639008092\n",
            "EPOCH 109 : RMSE ERROR : 11.232604672975082\n",
            "EPOCH 110 : RMSE ERROR : 11.23225317612275\n",
            "EPOCH 111 : RMSE ERROR : 11.231903764386148\n",
            "EPOCH 112 : RMSE ERROR : 11.231553190818527\n",
            "EPOCH 113 : RMSE ERROR : 11.231201397249508\n",
            "EPOCH 114 : RMSE ERROR : 11.230846602468484\n",
            "EPOCH 115 : RMSE ERROR : 11.230488880640806\n",
            "EPOCH 116 : RMSE ERROR : 11.230128291803059\n",
            "EPOCH 117 : RMSE ERROR : 11.229758226346778\n",
            "EPOCH 118 : RMSE ERROR : 11.229378698960668\n",
            "EPOCH 119 : RMSE ERROR : 11.228987024984631\n",
            "EPOCH 120 : RMSE ERROR : 11.228573631165887\n",
            "EPOCH 121 : RMSE ERROR : 11.228131635201622\n",
            "EPOCH 122 : RMSE ERROR : 11.22764153762913\n",
            "EPOCH 123 : RMSE ERROR : 11.227105853693798\n",
            "EPOCH 124 : RMSE ERROR : 11.226483849815962\n",
            "EPOCH 125 : RMSE ERROR : 11.225743521878593\n",
            "EPOCH 126 : RMSE ERROR : 11.224817520880373\n",
            "EPOCH 127 : RMSE ERROR : 11.22362099248377\n",
            "EPOCH 128 : RMSE ERROR : 11.221935396172817\n",
            "EPOCH 129 : RMSE ERROR : 11.219388830985014\n",
            "EPOCH 130 : RMSE ERROR : 11.215570599041191\n",
            "EPOCH 131 : RMSE ERROR : 11.210124109884816\n",
            "EPOCH 132 : RMSE ERROR : 11.203722921435311\n",
            "EPOCH 133 : RMSE ERROR : 11.198360736092095\n",
            "EPOCH 134 : RMSE ERROR : 11.195258396021151\n",
            "EPOCH 135 : RMSE ERROR : 11.193572698032044\n",
            "EPOCH 136 : RMSE ERROR : 11.19248390624423\n",
            "EPOCH 137 : RMSE ERROR : 11.191657201614909\n",
            "EPOCH 138 : RMSE ERROR : 11.19094153901873\n",
            "EPOCH 139 : RMSE ERROR : 11.19027905537448\n",
            "EPOCH 140 : RMSE ERROR : 11.189636597546908\n",
            "EPOCH 141 : RMSE ERROR : 11.189003136870951\n",
            "EPOCH 142 : RMSE ERROR : 11.188374529475482\n",
            "EPOCH 143 : RMSE ERROR : 11.187747394233321\n",
            "EPOCH 144 : RMSE ERROR : 11.18712184765328\n",
            "EPOCH 145 : RMSE ERROR : 11.186495347950185\n",
            "EPOCH 146 : RMSE ERROR : 11.185868552553547\n",
            "EPOCH 147 : RMSE ERROR : 11.18524170740855\n",
            "EPOCH 148 : RMSE ERROR : 11.184614554808691\n",
            "EPOCH 149 : RMSE ERROR : 11.18398668926855\n",
            "EPOCH 150 : RMSE ERROR : 11.183358529646183\n",
            "EPOCH 151 : RMSE ERROR : 11.182729924048227\n",
            "EPOCH 152 : RMSE ERROR : 11.182100856634401\n",
            "EPOCH 153 : RMSE ERROR : 11.181471283216784\n",
            "EPOCH 154 : RMSE ERROR : 11.180841192044202\n",
            "EPOCH 155 : RMSE ERROR : 11.180210766753527\n",
            "EPOCH 156 : RMSE ERROR : 11.179580091683748\n",
            "EPOCH 157 : RMSE ERROR : 11.17894911740702\n",
            "EPOCH 158 : RMSE ERROR : 11.178317852602808\n",
            "EPOCH 159 : RMSE ERROR : 11.177686412375142\n",
            "EPOCH 160 : RMSE ERROR : 11.17705477349119\n",
            "EPOCH 161 : RMSE ERROR : 11.176422850562552\n",
            "EPOCH 162 : RMSE ERROR : 11.175790624985321\n",
            "EPOCH 163 : RMSE ERROR : 11.175158039573134\n",
            "EPOCH 164 : RMSE ERROR : 11.174525051154259\n",
            "EPOCH 165 : RMSE ERROR : 11.173891557398306\n",
            "EPOCH 166 : RMSE ERROR : 11.17325748939035\n",
            "EPOCH 167 : RMSE ERROR : 11.17262247167303\n",
            "EPOCH 168 : RMSE ERROR : 11.17198624915252\n",
            "EPOCH 169 : RMSE ERROR : 11.171348328501336\n",
            "EPOCH 170 : RMSE ERROR : 11.170708279077216\n",
            "EPOCH 171 : RMSE ERROR : 11.170065252301374\n",
            "EPOCH 172 : RMSE ERROR : 11.16941738939261\n",
            "EPOCH 173 : RMSE ERROR : 11.168762584903712\n",
            "EPOCH 174 : RMSE ERROR : 11.16809711237824\n",
            "EPOCH 175 : RMSE ERROR : 11.167413894552228\n",
            "EPOCH 176 : RMSE ERROR : 11.166702416193727\n",
            "EPOCH 177 : RMSE ERROR : 11.165948656709975\n",
            "EPOCH 178 : RMSE ERROR : 11.165137497979831\n",
            "EPOCH 179 : RMSE ERROR : 11.164252328253\n",
            "EPOCH 180 : RMSE ERROR : 11.163276718567166\n",
            "EPOCH 181 : RMSE ERROR : 11.16220607781291\n",
            "EPOCH 182 : RMSE ERROR : 11.161054024295186\n",
            "EPOCH 183 : RMSE ERROR : 11.159833965987087\n",
            "EPOCH 184 : RMSE ERROR : 11.15856131675327\n",
            "EPOCH 185 : RMSE ERROR : 11.157280567604207\n",
            "EPOCH 186 : RMSE ERROR : 11.156026168067806\n",
            "EPOCH 187 : RMSE ERROR : 11.154822131278857\n",
            "EPOCH 188 : RMSE ERROR : 11.153694226013082\n",
            "EPOCH 189 : RMSE ERROR : 11.152652192142636\n",
            "EPOCH 190 : RMSE ERROR : 11.151676308408275\n",
            "EPOCH 191 : RMSE ERROR : 11.150759018166362\n",
            "EPOCH 192 : RMSE ERROR : 11.149875499790058\n",
            "EPOCH 193 : RMSE ERROR : 11.149020079906899\n",
            "EPOCH 194 : RMSE ERROR : 11.14818107499458\n",
            "EPOCH 195 : RMSE ERROR : 11.14735041551165\n",
            "EPOCH 196 : RMSE ERROR : 11.146526873231362\n",
            "EPOCH 197 : RMSE ERROR : 11.145707759193831\n",
            "EPOCH 198 : RMSE ERROR : 11.14489017921328\n",
            "EPOCH 199 : RMSE ERROR : 11.144073446186566\n",
            "EPOCH 200 : RMSE ERROR : 11.143257067376148\n",
            "EPOCH 201 : RMSE ERROR : 11.142440074929558\n",
            "EPOCH 202 : RMSE ERROR : 11.14162242734672\n",
            "EPOCH 203 : RMSE ERROR : 11.140804423208966\n",
            "EPOCH 204 : RMSE ERROR : 11.139985911567306\n",
            "EPOCH 205 : RMSE ERROR : 11.139166591680164\n",
            "EPOCH 206 : RMSE ERROR : 11.13834567303141\n",
            "EPOCH 207 : RMSE ERROR : 11.137523339157584\n",
            "EPOCH 208 : RMSE ERROR : 11.136700231814006\n",
            "EPOCH 209 : RMSE ERROR : 11.13587611480535\n",
            "EPOCH 210 : RMSE ERROR : 11.135050557814779\n",
            "EPOCH 211 : RMSE ERROR : 11.134223325995913\n",
            "EPOCH 212 : RMSE ERROR : 11.133394822912255\n",
            "EPOCH 213 : RMSE ERROR : 11.132564755173172\n",
            "EPOCH 214 : RMSE ERROR : 11.131733593692832\n",
            "EPOCH 215 : RMSE ERROR : 11.130901459487575\n",
            "EPOCH 216 : RMSE ERROR : 11.130068124802413\n",
            "EPOCH 217 : RMSE ERROR : 11.12923371745102\n",
            "EPOCH 218 : RMSE ERROR : 11.12839767930724\n",
            "EPOCH 219 : RMSE ERROR : 11.127560283904502\n",
            "EPOCH 220 : RMSE ERROR : 11.126721995070431\n",
            "EPOCH 221 : RMSE ERROR : 11.125882038150726\n",
            "EPOCH 222 : RMSE ERROR : 11.125039815139518\n",
            "EPOCH 223 : RMSE ERROR : 11.124195760432663\n",
            "EPOCH 224 : RMSE ERROR : 11.123349804358426\n",
            "EPOCH 225 : RMSE ERROR : 11.122501093799125\n",
            "EPOCH 226 : RMSE ERROR : 11.121650739155381\n",
            "EPOCH 227 : RMSE ERROR : 11.12079950597538\n",
            "EPOCH 228 : RMSE ERROR : 11.119945556151686\n",
            "EPOCH 229 : RMSE ERROR : 11.119089060885615\n",
            "EPOCH 230 : RMSE ERROR : 11.118229775820662\n",
            "EPOCH 231 : RMSE ERROR : 11.117368003584284\n",
            "EPOCH 232 : RMSE ERROR : 11.116504978250255\n",
            "EPOCH 233 : RMSE ERROR : 11.11563913939096\n",
            "EPOCH 234 : RMSE ERROR : 11.114769559180006\n",
            "EPOCH 235 : RMSE ERROR : 11.11389627386918\n",
            "EPOCH 236 : RMSE ERROR : 11.113017572202097\n",
            "EPOCH 237 : RMSE ERROR : 11.112133590657614\n",
            "EPOCH 238 : RMSE ERROR : 11.111243233213507\n",
            "EPOCH 239 : RMSE ERROR : 11.110346747556132\n",
            "EPOCH 240 : RMSE ERROR : 11.109443878522244\n",
            "EPOCH 241 : RMSE ERROR : 11.108533390768546\n",
            "EPOCH 242 : RMSE ERROR : 11.107616349204294\n",
            "EPOCH 243 : RMSE ERROR : 11.106691346824089\n",
            "EPOCH 244 : RMSE ERROR : 11.105755813240366\n",
            "EPOCH 245 : RMSE ERROR : 11.104808329684255\n",
            "EPOCH 246 : RMSE ERROR : 11.103842375438939\n",
            "EPOCH 247 : RMSE ERROR : 11.102856463163073\n",
            "EPOCH 248 : RMSE ERROR : 11.101852838809279\n",
            "EPOCH 249 : RMSE ERROR : 11.100821287306902\n",
            "EPOCH 250 : RMSE ERROR : 11.099756755440055\n",
            "EPOCH 251 : RMSE ERROR : 11.098664417088877\n",
            "EPOCH 252 : RMSE ERROR : 11.097541147254496\n",
            "EPOCH 253 : RMSE ERROR : 11.096377180401374\n",
            "EPOCH 254 : RMSE ERROR : 11.095173861547464\n",
            "EPOCH 255 : RMSE ERROR : 11.093914945279089\n",
            "EPOCH 256 : RMSE ERROR : 11.092608528635049\n",
            "EPOCH 257 : RMSE ERROR : 11.091252884344408\n",
            "EPOCH 258 : RMSE ERROR : 11.089871417979317\n",
            "EPOCH 259 : RMSE ERROR : 11.088475239714667\n",
            "EPOCH 260 : RMSE ERROR : 11.087092328896933\n",
            "EPOCH 261 : RMSE ERROR : 11.085744458450344\n",
            "EPOCH 262 : RMSE ERROR : 11.084457324329842\n",
            "EPOCH 263 : RMSE ERROR : 11.083238576801666\n",
            "EPOCH 264 : RMSE ERROR : 11.08207018349323\n",
            "EPOCH 265 : RMSE ERROR : 11.08095040991776\n",
            "EPOCH 266 : RMSE ERROR : 11.079873788065672\n",
            "EPOCH 267 : RMSE ERROR : 11.078826421083699\n",
            "EPOCH 268 : RMSE ERROR : 11.077802608817937\n",
            "EPOCH 269 : RMSE ERROR : 11.076798550793006\n",
            "EPOCH 270 : RMSE ERROR : 11.075802190459443\n",
            "EPOCH 271 : RMSE ERROR : 11.074813891183334\n",
            "EPOCH 272 : RMSE ERROR : 11.07383035157193\n",
            "EPOCH 273 : RMSE ERROR : 11.072847521288919\n",
            "EPOCH 274 : RMSE ERROR : 11.071866506231643\n",
            "EPOCH 275 : RMSE ERROR : 11.070887159254989\n",
            "EPOCH 276 : RMSE ERROR : 11.069908983086096\n",
            "EPOCH 277 : RMSE ERROR : 11.068929040286088\n",
            "EPOCH 278 : RMSE ERROR : 11.067948087920001\n",
            "EPOCH 279 : RMSE ERROR : 11.066965631697501\n",
            "EPOCH 280 : RMSE ERROR : 11.065980567500981\n",
            "EPOCH 281 : RMSE ERROR : 11.064993009132662\n",
            "EPOCH 282 : RMSE ERROR : 11.064003338380315\n",
            "EPOCH 283 : RMSE ERROR : 11.063010630726469\n",
            "EPOCH 284 : RMSE ERROR : 11.062015184108915\n",
            "EPOCH 285 : RMSE ERROR : 11.061017824607415\n",
            "EPOCH 286 : RMSE ERROR : 11.060018068576003\n",
            "EPOCH 287 : RMSE ERROR : 11.059017497014276\n",
            "EPOCH 288 : RMSE ERROR : 11.058016568908945\n",
            "EPOCH 289 : RMSE ERROR : 11.057012634896598\n",
            "EPOCH 290 : RMSE ERROR : 11.056005134374836\n",
            "EPOCH 291 : RMSE ERROR : 11.054993909729129\n",
            "EPOCH 292 : RMSE ERROR : 11.053981260871792\n",
            "EPOCH 293 : RMSE ERROR : 11.052965688090707\n",
            "EPOCH 294 : RMSE ERROR : 11.051945859543048\n",
            "EPOCH 295 : RMSE ERROR : 11.050922860205466\n",
            "EPOCH 296 : RMSE ERROR : 11.049896601943395\n",
            "EPOCH 297 : RMSE ERROR : 11.048864424070002\n",
            "EPOCH 298 : RMSE ERROR : 11.047828153813699\n",
            "EPOCH 299 : RMSE ERROR : 11.046780559593468\n",
            "EPOCH 300 : RMSE ERROR : 11.045718704895908\n",
            "EPOCH 301 : RMSE ERROR : 11.044648396615688\n",
            "EPOCH 302 : RMSE ERROR : 11.04354579065777\n",
            "EPOCH 303 : RMSE ERROR : 11.0423658760421\n",
            "EPOCH 304 : RMSE ERROR : 11.04107263359652\n",
            "EPOCH 305 : RMSE ERROR : 11.039660252920765\n",
            "EPOCH 306 : RMSE ERROR : 11.038270693047235\n",
            "EPOCH 307 : RMSE ERROR : 11.036935508884309\n",
            "EPOCH 308 : RMSE ERROR : 11.035592670833648\n",
            "EPOCH 309 : RMSE ERROR : 11.034194913875815\n",
            "EPOCH 310 : RMSE ERROR : 11.03270670373753\n",
            "EPOCH 311 : RMSE ERROR : 11.031141196304999\n",
            "EPOCH 312 : RMSE ERROR : 11.029484967933614\n",
            "EPOCH 313 : RMSE ERROR : 11.027779011885295\n",
            "EPOCH 314 : RMSE ERROR : 11.026043456280854\n",
            "EPOCH 315 : RMSE ERROR : 11.024351502559973\n",
            "EPOCH 316 : RMSE ERROR : 11.02279110205411\n",
            "EPOCH 317 : RMSE ERROR : 11.0213448898267\n",
            "EPOCH 318 : RMSE ERROR : 11.019956453781205\n",
            "EPOCH 319 : RMSE ERROR : 11.018585136217787\n",
            "EPOCH 320 : RMSE ERROR : 11.01720983863093\n",
            "EPOCH 321 : RMSE ERROR : 11.015815560160831\n",
            "EPOCH 322 : RMSE ERROR : 11.01439275080599\n",
            "EPOCH 323 : RMSE ERROR : 11.012918769112776\n",
            "EPOCH 324 : RMSE ERROR : 11.011364660719572\n",
            "EPOCH 325 : RMSE ERROR : 11.009688112074034\n",
            "EPOCH 326 : RMSE ERROR : 11.007823515861226\n",
            "EPOCH 327 : RMSE ERROR : 11.005775299688073\n",
            "EPOCH 328 : RMSE ERROR : 11.003709442620893\n",
            "EPOCH 329 : RMSE ERROR : 11.001781573222852\n",
            "EPOCH 330 : RMSE ERROR : 10.999981292065259\n",
            "EPOCH 331 : RMSE ERROR : 10.99824384745388\n",
            "EPOCH 332 : RMSE ERROR : 10.99654893998566\n",
            "EPOCH 333 : RMSE ERROR : 10.994881445861092\n",
            "EPOCH 334 : RMSE ERROR : 10.99322945856398\n",
            "EPOCH 335 : RMSE ERROR : 10.991587663497718\n",
            "EPOCH 336 : RMSE ERROR : 10.98995039409956\n",
            "EPOCH 337 : RMSE ERROR : 10.98831341781234\n",
            "EPOCH 338 : RMSE ERROR : 10.986674489379816\n",
            "EPOCH 339 : RMSE ERROR : 10.985033261577094\n",
            "EPOCH 340 : RMSE ERROR : 10.983388305989834\n",
            "EPOCH 341 : RMSE ERROR : 10.98173800861153\n",
            "EPOCH 342 : RMSE ERROR : 10.980081807556934\n",
            "EPOCH 343 : RMSE ERROR : 10.978417617349697\n",
            "EPOCH 344 : RMSE ERROR : 10.976744272324161\n",
            "EPOCH 345 : RMSE ERROR : 10.975060578129188\n",
            "EPOCH 346 : RMSE ERROR : 10.973366067352746\n",
            "EPOCH 347 : RMSE ERROR : 10.971661946408963\n",
            "EPOCH 348 : RMSE ERROR : 10.969945546637524\n",
            "EPOCH 349 : RMSE ERROR : 10.96821789144858\n",
            "EPOCH 350 : RMSE ERROR : 10.966479027713195\n",
            "EPOCH 351 : RMSE ERROR : 10.964738169084825\n",
            "EPOCH 352 : RMSE ERROR : 10.96299221674043\n",
            "EPOCH 353 : RMSE ERROR : 10.961245686855893\n",
            "EPOCH 354 : RMSE ERROR : 10.959494368869446\n",
            "EPOCH 355 : RMSE ERROR : 10.957736057074259\n",
            "EPOCH 356 : RMSE ERROR : 10.955966488145586\n",
            "EPOCH 357 : RMSE ERROR : 10.954187721097473\n",
            "EPOCH 358 : RMSE ERROR : 10.952399661230888\n",
            "EPOCH 359 : RMSE ERROR : 10.95060262511283\n",
            "EPOCH 360 : RMSE ERROR : 10.948796558926384\n",
            "EPOCH 361 : RMSE ERROR : 10.946982815364011\n",
            "EPOCH 362 : RMSE ERROR : 10.945162191620357\n",
            "EPOCH 363 : RMSE ERROR : 10.943334274825116\n",
            "EPOCH 364 : RMSE ERROR : 10.941490820079911\n",
            "EPOCH 365 : RMSE ERROR : 10.939629404096541\n",
            "EPOCH 366 : RMSE ERROR : 10.937746026162587\n",
            "EPOCH 367 : RMSE ERROR : 10.93583904623048\n",
            "EPOCH 368 : RMSE ERROR : 10.933893770342356\n",
            "EPOCH 369 : RMSE ERROR : 10.93191870413392\n",
            "EPOCH 370 : RMSE ERROR : 10.92992101440305\n",
            "EPOCH 371 : RMSE ERROR : 10.927887873098589\n",
            "EPOCH 372 : RMSE ERROR : 10.925805827003153\n",
            "EPOCH 373 : RMSE ERROR : 10.92364275702261\n",
            "EPOCH 374 : RMSE ERROR : 10.921375250991442\n",
            "EPOCH 375 : RMSE ERROR : 10.918964559585806\n",
            "EPOCH 376 : RMSE ERROR : 10.916410294650792\n",
            "EPOCH 377 : RMSE ERROR : 10.913721574382114\n",
            "EPOCH 378 : RMSE ERROR : 10.911003515964468\n",
            "EPOCH 379 : RMSE ERROR : 10.908356044139193\n",
            "EPOCH 380 : RMSE ERROR : 10.905881451962392\n",
            "EPOCH 381 : RMSE ERROR : 10.903536181073886\n",
            "EPOCH 382 : RMSE ERROR : 10.901207978679828\n",
            "EPOCH 383 : RMSE ERROR : 10.898882742868896\n",
            "EPOCH 384 : RMSE ERROR : 10.896554494180776\n",
            "EPOCH 385 : RMSE ERROR : 10.89420813838642\n",
            "EPOCH 386 : RMSE ERROR : 10.891854734550227\n",
            "EPOCH 387 : RMSE ERROR : 10.889502446515207\n",
            "EPOCH 388 : RMSE ERROR : 10.88714544608506\n",
            "EPOCH 389 : RMSE ERROR : 10.8847833383091\n",
            "EPOCH 390 : RMSE ERROR : 10.882415619338525\n",
            "EPOCH 391 : RMSE ERROR : 10.880042180800519\n",
            "EPOCH 392 : RMSE ERROR : 10.877660096621483\n",
            "EPOCH 393 : RMSE ERROR : 10.875269974422205\n",
            "EPOCH 394 : RMSE ERROR : 10.872873214541425\n",
            "EPOCH 395 : RMSE ERROR : 10.8704701554344\n",
            "EPOCH 396 : RMSE ERROR : 10.86806437891351\n",
            "EPOCH 397 : RMSE ERROR : 10.865655975846487\n",
            "EPOCH 398 : RMSE ERROR : 10.863241628674306\n",
            "EPOCH 399 : RMSE ERROR : 10.860819856205273\n",
            "EPOCH 400 : RMSE ERROR : 10.858392189534532\n",
            "EPOCH 401 : RMSE ERROR : 10.855959504826947\n",
            "EPOCH 402 : RMSE ERROR : 10.853522983528405\n",
            "EPOCH 403 : RMSE ERROR : 10.851083586484238\n",
            "EPOCH 404 : RMSE ERROR : 10.848640287614243\n",
            "EPOCH 405 : RMSE ERROR : 10.846193401988485\n",
            "EPOCH 406 : RMSE ERROR : 10.843744260816923\n",
            "EPOCH 407 : RMSE ERROR : 10.841291595004051\n",
            "EPOCH 408 : RMSE ERROR : 10.838835413460908\n",
            "EPOCH 409 : RMSE ERROR : 10.836377232206852\n",
            "EPOCH 410 : RMSE ERROR : 10.833915698670745\n",
            "EPOCH 411 : RMSE ERROR : 10.831450993939123\n",
            "EPOCH 412 : RMSE ERROR : 10.828983430867755\n",
            "EPOCH 413 : RMSE ERROR : 10.826514608668449\n",
            "EPOCH 414 : RMSE ERROR : 10.824044534552757\n",
            "EPOCH 415 : RMSE ERROR : 10.821573361420704\n",
            "EPOCH 416 : RMSE ERROR : 10.81910129070849\n",
            "EPOCH 417 : RMSE ERROR : 10.816628931685084\n",
            "EPOCH 418 : RMSE ERROR : 10.814153574050467\n",
            "EPOCH 419 : RMSE ERROR : 10.811675781889733\n",
            "EPOCH 420 : RMSE ERROR : 10.809197030606345\n",
            "EPOCH 421 : RMSE ERROR : 10.806718117723868\n",
            "EPOCH 422 : RMSE ERROR : 10.804238905679906\n",
            "EPOCH 423 : RMSE ERROR : 10.801760383648531\n",
            "EPOCH 424 : RMSE ERROR : 10.799281277911518\n",
            "EPOCH 425 : RMSE ERROR : 10.796801005645628\n",
            "EPOCH 426 : RMSE ERROR : 10.794320679340872\n",
            "EPOCH 427 : RMSE ERROR : 10.791840070865936\n",
            "EPOCH 428 : RMSE ERROR : 10.789358252828219\n",
            "EPOCH 429 : RMSE ERROR : 10.786874145230891\n",
            "EPOCH 430 : RMSE ERROR : 10.784387535948053\n",
            "EPOCH 431 : RMSE ERROR : 10.781899071715312\n",
            "EPOCH 432 : RMSE ERROR : 10.77940800168918\n",
            "EPOCH 433 : RMSE ERROR : 10.776915289090127\n",
            "EPOCH 434 : RMSE ERROR : 10.774422036066033\n",
            "EPOCH 435 : RMSE ERROR : 10.771928066295894\n",
            "EPOCH 436 : RMSE ERROR : 10.769434565173407\n",
            "EPOCH 437 : RMSE ERROR : 10.766940641255356\n",
            "EPOCH 438 : RMSE ERROR : 10.76444752875354\n",
            "EPOCH 439 : RMSE ERROR : 10.761953935298648\n",
            "EPOCH 440 : RMSE ERROR : 10.759460061033078\n",
            "EPOCH 441 : RMSE ERROR : 10.756965408949881\n",
            "EPOCH 442 : RMSE ERROR : 10.754470548449568\n",
            "EPOCH 443 : RMSE ERROR : 10.751975276393303\n",
            "EPOCH 444 : RMSE ERROR : 10.749480005197299\n",
            "EPOCH 445 : RMSE ERROR : 10.746985701453767\n",
            "EPOCH 446 : RMSE ERROR : 10.744491520913527\n",
            "EPOCH 447 : RMSE ERROR : 10.7419969712306\n",
            "EPOCH 448 : RMSE ERROR : 10.739501754646168\n",
            "EPOCH 449 : RMSE ERROR : 10.73700567255202\n",
            "EPOCH 450 : RMSE ERROR : 10.734507701821231\n",
            "EPOCH 451 : RMSE ERROR : 10.732008692687373\n",
            "EPOCH 452 : RMSE ERROR : 10.729508643253254\n",
            "EPOCH 453 : RMSE ERROR : 10.727005304486331\n",
            "EPOCH 454 : RMSE ERROR : 10.724504404610126\n",
            "EPOCH 455 : RMSE ERROR : 10.721998075441777\n",
            "EPOCH 456 : RMSE ERROR : 10.719482816019656\n",
            "EPOCH 457 : RMSE ERROR : 10.71695199527632\n",
            "EPOCH 458 : RMSE ERROR : 10.714392966423226\n",
            "EPOCH 459 : RMSE ERROR : 10.711794488937315\n",
            "EPOCH 460 : RMSE ERROR : 10.709168515356753\n",
            "EPOCH 461 : RMSE ERROR : 10.706565156431873\n",
            "EPOCH 462 : RMSE ERROR : 10.703990988940449\n",
            "EPOCH 463 : RMSE ERROR : 10.701429572959771\n",
            "EPOCH 464 : RMSE ERROR : 10.698869119023286\n",
            "EPOCH 465 : RMSE ERROR : 10.696304652116547\n",
            "EPOCH 466 : RMSE ERROR : 10.693734376047466\n",
            "EPOCH 467 : RMSE ERROR : 10.69115839953277\n",
            "EPOCH 468 : RMSE ERROR : 10.688577612252693\n",
            "EPOCH 469 : RMSE ERROR : 10.685992128590852\n",
            "EPOCH 470 : RMSE ERROR : 10.683402954997153\n",
            "EPOCH 471 : RMSE ERROR : 10.680809293526092\n",
            "EPOCH 472 : RMSE ERROR : 10.678211890217927\n",
            "EPOCH 473 : RMSE ERROR : 10.675610534422106\n",
            "EPOCH 474 : RMSE ERROR : 10.673006153014866\n",
            "EPOCH 475 : RMSE ERROR : 10.670397142922456\n",
            "EPOCH 476 : RMSE ERROR : 10.667784607173838\n",
            "EPOCH 477 : RMSE ERROR : 10.665169166400585\n",
            "EPOCH 478 : RMSE ERROR : 10.66255187075414\n",
            "EPOCH 479 : RMSE ERROR : 10.659933497195622\n",
            "EPOCH 480 : RMSE ERROR : 10.6573133276496\n",
            "EPOCH 481 : RMSE ERROR : 10.65469102685485\n",
            "EPOCH 482 : RMSE ERROR : 10.652067339557957\n",
            "EPOCH 483 : RMSE ERROR : 10.64944446373697\n",
            "EPOCH 484 : RMSE ERROR : 10.646819748344631\n",
            "EPOCH 485 : RMSE ERROR : 10.644193496872534\n",
            "EPOCH 486 : RMSE ERROR : 10.641569102316618\n",
            "EPOCH 487 : RMSE ERROR : 10.638945347507216\n",
            "EPOCH 488 : RMSE ERROR : 10.636321658115206\n",
            "EPOCH 489 : RMSE ERROR : 10.633697127389693\n",
            "EPOCH 490 : RMSE ERROR : 10.631071584735084\n",
            "EPOCH 491 : RMSE ERROR : 10.628444479768644\n",
            "EPOCH 492 : RMSE ERROR : 10.625815217219849\n",
            "EPOCH 493 : RMSE ERROR : 10.623183618246745\n",
            "EPOCH 494 : RMSE ERROR : 10.620550319402978\n",
            "EPOCH 495 : RMSE ERROR : 10.617915211775998\n",
            "EPOCH 496 : RMSE ERROR : 10.615278972964738\n",
            "EPOCH 497 : RMSE ERROR : 10.612645630093594\n",
            "EPOCH 498 : RMSE ERROR : 10.610010916653412\n",
            "EPOCH 499 : RMSE ERROR : 10.607373005608794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = FFNN(dev_X)\n",
        "dev_mse = loss_mse(dev_Y,predictions).iloc[0]\n",
        "print(f\"TRAIN SET : {mse_error} | DEV SET : {dev_mse}\")\n",
        "print(f\"TRAIN SET * DEV SET : {mse_error*dev_mse}\")"
      ],
      "metadata": {
        "id": "SZMlzp0jMxJG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48671ac1-c5e0-4965-d50b-a767722e5ee6"
      },
      "execution_count": 849,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN SET : 10.607373005608794 | DEV SET : 10.875862816613882\n",
            "TRAIN SET * DEV SET : 10875862816.613882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tst = FFNN(test_X)\n",
        "df = pd.DataFrame(tst)\n",
        "df.index = df.index+1\n",
        "df.to_csv('pred.csv', header=['Predictions'], index=True, index_label='Id')\n",
        "print(tst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zsvsaUwHEaX",
        "outputId": "bf4f4f70-e0c8-42f3-c405-8d2f26b92afe"
      },
      "execution_count": 853,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2000.10721681]\n",
            " [2000.96907962]\n",
            " [2001.89851139]\n",
            " ...\n",
            " [1999.47686756]\n",
            " [2000.52366503]\n",
            " [1998.0668595 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tst.min()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBblDlyD-zgq",
        "outputId": "3f3bb999-606c-4d16-de9c-f0e5fe773883"
      },
      "execution_count": 854,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1987.7346258288103"
            ]
          },
          "metadata": {},
          "execution_count": 854
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tst.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BAComdvlsGN",
        "outputId": "00fdacd7-4b2c-43fe-de6d-2a890e9bab7c"
      },
      "execution_count": 855,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2011.5280119934973"
            ]
          },
          "metadata": {},
          "execution_count": 855
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oKGA3kzLavXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}